{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM-VMNet Showcase: Visual Demo of Model Predictions\n",
    "\n",
    "This notebook demonstrates the SAM-VMNet / VMUNet model on angiography vessel segmentation.\n",
    "We identify which checkpoint belongs to which architecture, load the best model,\n",
    "and visualize predictions on sample test images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from utils import set_seed, BceDiceLoss\n",
    "from configs.config_setting import setting_config\n",
    "from models.vmunet.vmunet import VMUNet\n",
    "from models.vmunet.samvmnet import SAMVMNet\n",
    "from dataset import Branch1_datasets\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Auto-detect GPU and override config before model construction\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    setting_config.gpu_id = '0'\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Identify Checkpoints\n",
    "\n",
    "Test-load each checkpoint into VMUNet and SAMVMNet with `strict=True` to determine which architecture each belongs to."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ckpt_paths = {\n",
    "    'loss0.3230 (105MB)': './pre_trained_weights/best-epoch142-loss0.3230.pth',\n",
    "    'loss0.3488 (112MB)': './pre_trained_weights/best-epoch142-loss0.3488.pth',\n",
    "}\n",
    "\n",
    "model_cfg = setting_config.model_config\n",
    "results = {}\n",
    "\n",
    "for ckpt_name, ckpt_path in ckpt_paths.items():\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    filtered = {k: v for k, v in checkpoint.items()\n",
    "                if 'total_ops' not in k and 'total_params' not in k}\n",
    "    results[ckpt_name] = []\n",
    "\n",
    "    # Try VMUNet (Branch 1)\n",
    "    try:\n",
    "        model_test = VMUNet(\n",
    "            num_classes=model_cfg['num_classes'],\n",
    "            input_channels=model_cfg['input_channels'],\n",
    "            depths=model_cfg['depths'],\n",
    "            depths_decoder=model_cfg['depths_decoder'],\n",
    "            drop_path_rate=model_cfg['drop_path_rate'],\n",
    "            load_ckpt_path=None,\n",
    "        )\n",
    "        model_test.load_state_dict(filtered, strict=True)\n",
    "        results[ckpt_name].append('VMUNet')\n",
    "        print(f\"  [OK] {ckpt_name} is compatible with VMUNet\")\n",
    "        del model_test\n",
    "    except Exception as e:\n",
    "        print(f\"  [--] {ckpt_name} NOT compatible with VMUNet: {str(e)[:120]}\")\n",
    "\n",
    "    # Try SAMVMNet (Branch 2)\n",
    "    try:\n",
    "        model_test = SAMVMNet(\n",
    "            num_classes=model_cfg['num_classes'],\n",
    "            input_channels=model_cfg['input_channels'],\n",
    "            depths=model_cfg['depths'],\n",
    "            depths_decoder=model_cfg['depths_decoder'],\n",
    "            drop_path_rate=model_cfg['drop_path_rate'],\n",
    "            load_ckpt_path=None,\n",
    "        )\n",
    "        model_test.load_state_dict(filtered, strict=True)\n",
    "        results[ckpt_name].append('SAMVMNet')\n",
    "        print(f\"  [OK] {ckpt_name} is compatible with SAMVMNet\")\n",
    "        del model_test\n",
    "    except Exception as e:\n",
    "        print(f\"  [--] {ckpt_name} NOT compatible with SAMVMNet: {str(e)[:120]}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n=== Checkpoint Identity Summary ===\")\n",
    "for ckpt_name, archs in results.items():\n",
    "    arch_str = ', '.join(archs) if archs else 'UNKNOWN'\n",
    "    print(f\"  {ckpt_name}: {arch_str}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Best VMUNet Model\n",
    "\n",
    "Load the identified Branch 1 (VMUNet) checkpoint. We use the lower-loss checkpoint (`loss0.3230`).\n",
    "Adjust `CKPT_PATH` below if the identification step reveals a different mapping."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Adjust this path based on the identification results above ---\n",
    "CKPT_PATH = './pre_trained_weights/best-epoch142-loss0.3230.pth'\n",
    "\n",
    "model_cfg = setting_config.model_config\n",
    "model = VMUNet(\n",
    "    num_classes=model_cfg['num_classes'],\n",
    "    input_channels=model_cfg['input_channels'],\n",
    "    depths=model_cfg['depths'],\n",
    "    depths_decoder=model_cfg['depths_decoder'],\n",
    "    drop_path_rate=model_cfg['drop_path_rate'],\n",
    "    load_ckpt_path=model_cfg['load_ckpt_path'],\n",
    ")\n",
    "model.load_from()\n",
    "\n",
    "checkpoint = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "filtered_state_dict = {k: v for k, v in checkpoint.items()\n",
    "                       if 'total_ops' not in k and 'total_params' not in k}\n",
    "model.load_state_dict(filtered_state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded from {CKPT_PATH}\")\n",
    "print(f\"Model device: {model.device}\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params/1e6:.2f}M\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Sample Test Images\n",
    "\n",
    "Pick 8 evenly-spaced images from the test set to show diverse examples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "DATA_PATH = './data/vessel/'\n",
    "\n",
    "test_dataset = Branch1_datasets(DATA_PATH, setting_config, train=False, test=True)\n",
    "print(f\"Test set size: {len(test_dataset)} images\")\n",
    "\n",
    "# Pick 8 evenly-spaced indices\n",
    "num_samples = 8\n",
    "sample_indices = np.linspace(0, len(test_dataset) - 1, num_samples, dtype=int)\n",
    "print(f\"Sample indices: {sample_indices}\")\n",
    "\n",
    "# Load transformed images (for inference) and raw images (for display)\n",
    "sample_imgs = []      # transformed tensors\n",
    "sample_msks = []      # transformed mask tensors\n",
    "sample_raw_imgs = []  # raw PIL images for display\n",
    "sample_raw_msks = []  # raw masks for display\n",
    "sample_names = []\n",
    "\n",
    "for idx in sample_indices:\n",
    "    img_tensor, msk_tensor = test_dataset[idx]\n",
    "    sample_imgs.append(img_tensor)\n",
    "    sample_msks.append(msk_tensor)\n",
    "\n",
    "    # Load raw for display\n",
    "    img_path, msk_path = test_dataset.data[idx]\n",
    "    sample_raw_imgs.append(np.array(Image.open(img_path).convert('RGB')))\n",
    "    sample_raw_msks.append(np.array(Image.open(msk_path).convert('L')))\n",
    "    sample_names.append(os.path.basename(img_path))\n",
    "\n",
    "# Display the raw images in a grid\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(sample_raw_imgs[i])\n",
    "    ax.set_title(sample_names[i], fontsize=10)\n",
    "    ax.axis('off')\n",
    "fig.suptitle('Selected Test Images', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "threshold = setting_config.threshold  # 0.5\n",
    "predictions = []\n",
    "pred_probs = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img_tensor in sample_imgs:\n",
    "        # Add batch dimension: (C, H, W) -> (1, C, H, W)\n",
    "        x = img_tensor.unsqueeze(0).float()\n",
    "        out = model(x)  # model moves input to its device internally\n",
    "        out = out.squeeze().cpu().numpy()  # (H, W) probability map\n",
    "        pred_probs.append(out)\n",
    "        pred_binary = (out >= threshold).astype(np.float32)\n",
    "        predictions.append(pred_binary)\n",
    "\n",
    "print(f\"Inference complete on {len(predictions)} images\")\n",
    "print(f\"Prediction shape: {predictions[0].shape}\")\n",
    "print(f\"Probability range: [{pred_probs[0].min():.4f}, {pred_probs[0].max():.4f}]\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 3-Panel Visualization (Original | Ground Truth | Prediction)\n",
    "\n",
    "For each sample, show the original image, ground truth mask, and predicted mask side by side,\n",
    "with per-image Dice score annotated."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_dice(pred, gt):\n",
    "    \"\"\"Compute Dice score between binary prediction and ground truth.\"\"\"\n",
    "    pred_flat = pred.flatten()\n",
    "    gt_flat = gt.flatten()\n",
    "    intersection = np.sum(pred_flat * gt_flat)\n",
    "    if (pred_flat.sum() + gt_flat.sum()) == 0:\n",
    "        return 1.0\n",
    "    return (2.0 * intersection) / (pred_flat.sum() + gt_flat.sum())\n",
    "\n",
    "\n",
    "def compute_miou(pred, gt):\n",
    "    \"\"\"Compute mIoU between binary prediction and ground truth.\"\"\"\n",
    "    pred_flat = pred.flatten()\n",
    "    gt_flat = gt.flatten()\n",
    "    intersection = np.sum(pred_flat * gt_flat)\n",
    "    union = pred_flat.sum() + gt_flat.sum() - intersection\n",
    "    if union == 0:\n",
    "        return 1.0\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(num_samples, 3, figsize=(14, 4 * num_samples))\n",
    "\n",
    "dice_scores = []\n",
    "miou_scores = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Ground truth: resize raw mask to 256x256 to match prediction\n",
    "    gt_resized = np.array(Image.fromarray(sample_raw_msks[i]).resize((256, 256), Image.NEAREST))\n",
    "    gt_binary = (gt_resized / 255.0 >= 0.5).astype(np.float32)\n",
    "\n",
    "    dice = compute_dice(predictions[i], gt_binary)\n",
    "    miou = compute_miou(predictions[i], gt_binary)\n",
    "    dice_scores.append(dice)\n",
    "    miou_scores.append(miou)\n",
    "\n",
    "    # Original image\n",
    "    axes[i, 0].imshow(sample_raw_imgs[i])\n",
    "    axes[i, 0].set_title(f'{sample_names[i]}', fontsize=10)\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    # Ground truth\n",
    "    axes[i, 1].imshow(gt_binary, cmap='gray')\n",
    "    axes[i, 1].set_title('Ground Truth', fontsize=10)\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    # Prediction\n",
    "    axes[i, 2].imshow(predictions[i], cmap='gray')\n",
    "    axes[i, 2].set_title(f'Prediction (Dice={dice:.4f})', fontsize=10)\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "fig.suptitle('Model Predictions: Original | Ground Truth | Prediction', fontsize=14, fontweight='bold', y=1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Overlay Visualization\n",
    "\n",
    "Overlay predicted vessels on the original image:\n",
    "- **Green**: True Positive (correctly predicted vessel)\n",
    "- **Red**: False Positive (incorrectly predicted vessel)\n",
    "- **Blue**: False Negative (missed vessel)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Resize raw image to 256x256\n",
    "    raw_resized = np.array(Image.fromarray(sample_raw_imgs[i]).resize((256, 256), Image.BILINEAR))\n",
    "    gt_resized = np.array(Image.fromarray(sample_raw_msks[i]).resize((256, 256), Image.NEAREST))\n",
    "    gt_binary = (gt_resized / 255.0 >= 0.5).astype(np.float32)\n",
    "\n",
    "    pred = predictions[i]\n",
    "\n",
    "    # Create overlay\n",
    "    overlay = raw_resized.copy().astype(np.float32)\n",
    "    alpha = 0.45\n",
    "\n",
    "    # True Positive -> green\n",
    "    tp_mask = (pred == 1) & (gt_binary == 1)\n",
    "    overlay[tp_mask] = overlay[tp_mask] * (1 - alpha) + np.array([0, 255, 0]) * alpha\n",
    "\n",
    "    # False Positive -> red\n",
    "    fp_mask = (pred == 1) & (gt_binary == 0)\n",
    "    overlay[fp_mask] = overlay[fp_mask] * (1 - alpha) + np.array([255, 0, 0]) * alpha\n",
    "\n",
    "    # False Negative -> blue\n",
    "    fn_mask = (pred == 0) & (gt_binary == 1)\n",
    "    overlay[fn_mask] = overlay[fn_mask] * (1 - alpha) + np.array([0, 100, 255]) * alpha\n",
    "\n",
    "    ax.imshow(overlay.astype(np.uint8))\n",
    "    ax.set_title(f'{sample_names[i]}\\nDice={dice_scores[i]:.4f}', fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "fig.suptitle('Overlay: Green=TP, Red=FP, Blue=FN', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Quick Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"Quick Summary (on 8 sample images)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Mean Dice:  {np.mean(dice_scores):.4f} +/- {np.std(dice_scores):.4f}\")\n",
    "print(f\"  Mean mIoU:  {np.mean(miou_scores):.4f} +/- {np.std(miou_scores):.4f}\")\n",
    "print()\n",
    "print(\"Per-image breakdown:\")\n",
    "print(f\"  {'Image':<15} {'Dice':>8} {'mIoU':>8}\")\n",
    "print(f\"  {'-'*15} {'-'*8} {'-'*8}\")\n",
    "for i in range(num_samples):\n",
    "    print(f\"  {sample_names[i]:<15} {dice_scores[i]:>8.4f} {miou_scores[i]:>8.4f}\")\n",
    "print(f\"  {'-'*15} {'-'*8} {'-'*8}\")\n",
    "print(f\"  {'Mean':<15} {np.mean(dice_scores):>8.4f} {np.mean(miou_scores):>8.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}