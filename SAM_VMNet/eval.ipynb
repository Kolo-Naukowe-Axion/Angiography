{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM-VMNet Full Evaluation on ARCADE Test Set\n",
    "\n",
    "Quantitative evaluation of the VMUNet model on the full 300-image test set.\n",
    "Computes mIoU, Dice/F1, accuracy, sensitivity, specificity, and visualizes results.\n",
    "\n",
    "Metrics computation matches `engine_branch1.py` exactly for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from utils import set_seed, BceDiceLoss\n",
    "from configs.config_setting import setting_config\n",
    "from models.vmunet.vmunet import VMUNet\n",
    "from dataset import Branch1_datasets\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Auto-detect GPU and override config before model construction\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    setting_config.gpu_id = '0'\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Load the VMUNet checkpoint (adjust `CKPT_PATH` if the identification in `showcase_model.ipynb` showed a different mapping)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "CKPT_PATH = './pre_trained_weights/best-epoch142-loss0.3230.pth'\n",
    "DATA_PATH = './data/vessel/'\n",
    "\n",
    "model_cfg = setting_config.model_config\n",
    "model = VMUNet(\n",
    "    num_classes=model_cfg['num_classes'],\n",
    "    input_channels=model_cfg['input_channels'],\n",
    "    depths=model_cfg['depths'],\n",
    "    depths_decoder=model_cfg['depths_decoder'],\n",
    "    drop_path_rate=model_cfg['drop_path_rate'],\n",
    "    load_ckpt_path=model_cfg['load_ckpt_path'],\n",
    ")\n",
    "model.load_from()\n",
    "\n",
    "checkpoint = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "filtered_state_dict = {k: v for k, v in checkpoint.items()\n",
    "                       if 'total_ops' not in k and 'total_params' not in k}\n",
    "model.load_state_dict(filtered_state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded from {CKPT_PATH}\")\n",
    "print(f\"Model device: {model.device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Full Test Set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_dataset = Branch1_datasets(DATA_PATH, setting_config, train=False, test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "print(f\"Test set: {len(test_dataset)} images\")\n",
    "print(f\"Batch size: 1, Total batches: {len(test_loader)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Evaluation\n",
    "\n",
    "Iterate all test images, collect predictions and ground truths.\n",
    "Store per-image predictions for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "threshold = setting_config.threshold  # 0.5\n",
    "criterion = setting_config.criterion\n",
    "\n",
    "all_preds = []   # per-image prediction arrays\n",
    "all_gts = []     # per-image ground truth arrays\n",
    "loss_list = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "        img, msk = data\n",
    "        img = img.float()\n",
    "        msk = msk.float()\n",
    "\n",
    "        # Model moves input to its device internally\n",
    "        out = model(img)\n",
    "        # Compute loss on same device as msk\n",
    "        loss = criterion(out.cpu(), msk)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        msk_np = msk.squeeze(1).cpu().detach().numpy()  # (1, H, W) -> (H, W)\n",
    "        if type(out) is tuple:\n",
    "            out = out[0]\n",
    "        out_np = out.squeeze(1).cpu().detach().numpy()   # (1, H, W) -> (H, W)\n",
    "\n",
    "        all_preds.append(out_np)\n",
    "        all_gts.append(msk_np)\n",
    "\n",
    "print(f\"\\nEvaluation complete. Mean loss: {np.mean(loss_list):.4f}\")\n",
    "print(f\"Collected predictions for {len(all_preds)} images\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Aggregate Metrics\n",
    "\n",
    "Replicate the exact metrics computation from `engine_branch1.py:144-157`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Flatten all predictions and ground truths (matches engine_branch1.py exactly)\n",
    "preds_flat = np.array(all_preds).reshape(-1)\n",
    "gts_flat = np.array(all_gts).reshape(-1)\n",
    "\n",
    "y_pre = np.where(preds_flat >= threshold, 1, 0)\n",
    "y_true = np.where(gts_flat >= 0.5, 1, 0)\n",
    "\n",
    "confusion = confusion_matrix(y_true, y_pre)\n",
    "TN, FP, FN, TP = confusion[0, 0], confusion[0, 1], confusion[1, 0], confusion[1, 1]\n",
    "\n",
    "accuracy = float(TN + TP) / float(np.sum(confusion)) if float(np.sum(confusion)) != 0 else 0\n",
    "sensitivity = float(TP) / float(TP + FN) if float(TP + FN) != 0 else 0\n",
    "specificity = float(TN) / float(TN + FP) if float(TN + FP) != 0 else 0\n",
    "f1_or_dsc = float(2 * TP) / float(2 * TP + FP + FN) if float(2 * TP + FP + FN) != 0 else 0\n",
    "miou = float(TP) / float(TP + FP + FN) if float(TP + FP + FN) != 0 else 0\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AGGREGATE METRICS (Full Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Loss:         {np.mean(loss_list):.4f}\")\n",
    "print(f\"  mIoU:         {miou:.4f}\")\n",
    "print(f\"  Dice/F1:      {f1_or_dsc:.4f}\")\n",
    "print(f\"  Accuracy:     {accuracy:.4f}\")\n",
    "print(f\"  Sensitivity:  {sensitivity:.4f}\")\n",
    "print(f\"  Specificity:  {specificity:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TN={TN:,}  FP={FP:,}\")\n",
    "print(f\"  FN={FN:,}  TP={TP:,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Image Metrics Distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "per_image_dice = []\n",
    "per_image_miou = []\n",
    "\n",
    "for i in range(len(all_preds)):\n",
    "    pred_flat = all_preds[i].reshape(-1)\n",
    "    gt_flat = all_gts[i].reshape(-1)\n",
    "\n",
    "    y_p = np.where(pred_flat >= threshold, 1, 0)\n",
    "    y_t = np.where(gt_flat >= 0.5, 1, 0)\n",
    "\n",
    "    cm = confusion_matrix(y_t, y_p, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
    "\n",
    "    dice = float(2 * tp) / float(2 * tp + fp + fn) if float(2 * tp + fp + fn) != 0 else 1.0\n",
    "    iou = float(tp) / float(tp + fp + fn) if float(tp + fp + fn) != 0 else 1.0\n",
    "\n",
    "    per_image_dice.append(dice)\n",
    "    per_image_miou.append(iou)\n",
    "\n",
    "per_image_dice = np.array(per_image_dice)\n",
    "per_image_miou = np.array(per_image_miou)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Dice histogram\n",
    "axes[0].hist(per_image_dice, bins=30, color='steelblue', edgecolor='white', alpha=0.85)\n",
    "axes[0].axvline(np.mean(per_image_dice), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean={np.mean(per_image_dice):.4f}')\n",
    "axes[0].axvline(np.median(per_image_dice), color='orange', linestyle='--', linewidth=2,\n",
    "                label=f'Median={np.median(per_image_dice):.4f}')\n",
    "axes[0].set_xlabel('Dice Score', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Per-Image Dice Score Distribution', fontsize=13)\n",
    "axes[0].legend(fontsize=10)\n",
    "\n",
    "# mIoU histogram\n",
    "axes[1].hist(per_image_miou, bins=30, color='darkorange', edgecolor='white', alpha=0.85)\n",
    "axes[1].axvline(np.mean(per_image_miou), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean={np.mean(per_image_miou):.4f}')\n",
    "axes[1].axvline(np.median(per_image_miou), color='blue', linestyle='--', linewidth=2,\n",
    "                label=f'Median={np.median(per_image_miou):.4f}')\n",
    "axes[1].set_xlabel('mIoU', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Per-Image mIoU Distribution', fontsize=13)\n",
    "axes[1].legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Per-image Dice: mean={np.mean(per_image_dice):.4f}, std={np.std(per_image_dice):.4f}, \"\n",
    "      f\"min={np.min(per_image_dice):.4f}, max={np.max(per_image_dice):.4f}\")\n",
    "print(f\"Per-image mIoU: mean={np.mean(per_image_miou):.4f}, std={np.std(per_image_miou):.4f}, \"\n",
    "      f\"min={np.min(per_image_miou):.4f}, max={np.max(per_image_miou):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "cm_display = np.array([[TN, FP], [FN, TP]])\n",
    "cm_normalized = cm_display.astype(float) / cm_display.sum()\n",
    "\n",
    "im = ax.imshow(cm_normalized, cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar(im, ax=ax, label='Proportion')\n",
    "\n",
    "labels = ['Background (0)', 'Vessel (1)']\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(labels, fontsize=11)\n",
    "ax.set_yticklabels(labels, fontsize=11)\n",
    "ax.set_xlabel('Predicted', fontsize=13)\n",
    "ax.set_ylabel('Actual', fontsize=13)\n",
    "ax.set_title('Confusion Matrix (Normalized)', fontsize=14)\n",
    "\n",
    "# Annotate cells with both count and percentage\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        count = cm_display[i, j]\n",
    "        pct = cm_normalized[i, j] * 100\n",
    "        color = 'white' if cm_normalized[i, j] > 0.5 else 'black'\n",
    "        ax.text(j, i, f'{count:,}\\n({pct:.2f}%)',\n",
    "                ha='center', va='center', fontsize=12, color=color, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"         FINAL EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Model:        VMUNet (Branch 1)\")\n",
    "print(f\"  Checkpoint:   {os.path.basename(CKPT_PATH)}\")\n",
    "print(f\"  Test images:  {len(test_dataset)}\")\n",
    "print(f\"  Threshold:    {threshold}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  {'Metric':<20} {'Value':>10}\")\n",
    "print(f\"  {'-'*20} {'-'*10}\")\n",
    "print(f\"  {'Loss':<20} {np.mean(loss_list):>10.4f}\")\n",
    "print(f\"  {'mIoU':<20} {miou:>10.4f}\")\n",
    "print(f\"  {'Dice / F1':<20} {f1_or_dsc:>10.4f}\")\n",
    "print(f\"  {'Accuracy':<20} {accuracy:>10.4f}\")\n",
    "print(f\"  {'Sensitivity':<20} {sensitivity:>10.4f}\")\n",
    "print(f\"  {'Specificity':<20} {specificity:>10.4f}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  {'Per-Image Dice':<20} {np.mean(per_image_dice):>10.4f} +/- {np.std(per_image_dice):.4f}\")\n",
    "print(f\"  {'Per-Image mIoU':<20} {np.mean(per_image_miou):>10.4f} +/- {np.std(per_image_miou):.4f}\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best and Worst Cases\n",
    "\n",
    "Show the 3 best and 3 worst predictions by Dice score."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sorted_indices = np.argsort(per_image_dice)\n",
    "worst_3 = sorted_indices[:3]\n",
    "best_3 = sorted_indices[-3:][::-1]\n",
    "\n",
    "def show_cases(indices, title):\n",
    "    fig, axes = plt.subplots(len(indices), 3, figsize=(14, 4.5 * len(indices)))\n",
    "    if len(indices) == 1:\n",
    "        axes = axes[np.newaxis, :]\n",
    "\n",
    "    for row, idx in enumerate(indices):\n",
    "        img_path, msk_path = test_dataset.data[idx]\n",
    "        raw_img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        raw_msk = np.array(Image.open(msk_path).convert('L'))\n",
    "\n",
    "        gt_resized = np.array(Image.fromarray(raw_msk).resize((256, 256), Image.NEAREST))\n",
    "        gt_binary = (gt_resized / 255.0 >= 0.5).astype(np.float32)\n",
    "        pred_binary = np.where(all_preds[idx].squeeze() >= threshold, 1, 0).astype(np.float32)\n",
    "\n",
    "        # Original\n",
    "        axes[row, 0].imshow(raw_img)\n",
    "        axes[row, 0].set_title(f'{os.path.basename(img_path)}', fontsize=10)\n",
    "        axes[row, 0].axis('off')\n",
    "\n",
    "        # Ground truth\n",
    "        axes[row, 1].imshow(gt_binary, cmap='gray')\n",
    "        axes[row, 1].set_title('Ground Truth', fontsize=10)\n",
    "        axes[row, 1].axis('off')\n",
    "\n",
    "        # Prediction\n",
    "        axes[row, 2].imshow(pred_binary, cmap='gray')\n",
    "        axes[row, 2].set_title(f'Prediction (Dice={per_image_dice[idx]:.4f})', fontsize=10)\n",
    "        axes[row, 2].axis('off')\n",
    "\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold', y=1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_cases(best_3, 'Top 3 Best Predictions (Highest Dice)')\n",
    "show_cases(worst_3, 'Top 3 Worst Predictions (Lowest Dice)')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}