{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfFqSdFgqR+cW4WFohDCNc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kolo-Naukowe-Axion/Angiography/blob/main/arcade_cadica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NGePR-DdIrN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import shutil\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import kagglehub\n",
        "\n",
        "# ==========================================\n",
        "# 0. SETUP & HELPER FUNCTIONS\n",
        "# ==========================================\n",
        "print(\"--- INITIALIZING FUSION PIPELINE ---\")\n",
        "\n",
        "# 1. Folder with all data (background + stenosis)\n",
        "DIR_ALL = Path('fused_dataset_all')\n",
        "IMG_ALL = DIR_ALL / 'images'\n",
        "LBL_ALL = DIR_ALL / 'labels'\n",
        "\n",
        "# 2. Folder with stenosis only\n",
        "DIR_STENOSIS = Path('fused_dataset_stenosis_only')\n",
        "IMG_STENOSIS = DIR_STENOSIS / 'images'\n",
        "LBL_STENOSIS = DIR_STENOSIS / 'labels'\n",
        "\n",
        "# 3. Folder with background only (NO stenosis)\n",
        "DIR_NO_STENOSIS = Path('fused_dataset_no_stenosis')\n",
        "IMG_NO_STENOSIS = DIR_NO_STENOSIS / 'images'\n",
        "LBL_NO_STENOSIS = DIR_NO_STENOSIS / 'labels'\n",
        "\n",
        "# Create all necessary folders\n",
        "for p in [IMG_ALL, LBL_ALL, IMG_STENOSIS, LBL_STENOSIS, IMG_NO_STENOSIS, LBL_NO_STENOSIS]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def run_cmd(cmd):\n",
        "    subprocess.run(cmd, shell=True, check=True)\n",
        "\n",
        "def to_yolo(img_w, img_h, x_min, y_min, box_w, box_h):\n",
        "    x_center = (x_min + (box_w / 2.0)) / img_w\n",
        "    y_center = (y_min + (box_h / 2.0)) / img_h\n",
        "    norm_w = box_w / img_w\n",
        "    norm_h = box_h / img_h\n",
        "    return x_center, y_center, norm_w, norm_h\n",
        "\n",
        "def save_image_and_labels(src_img_path, img_target_dir, lbl_target_dir, new_img_name, new_lbl_name, annotations):\n",
        "    \"\"\"Helper function to save image and its YOLO labels to specific directories.\"\"\"\n",
        "    shutil.copy(src_img_path, img_target_dir / new_img_name)\n",
        "    with open(lbl_target_dir / new_lbl_name, 'w') as f_out:\n",
        "        f_out.writelines(annotations)\n",
        "\n",
        "# ==========================================\n",
        "# 1. DOWNLOAD & PROCESS CADICA\n",
        "# ==========================================\n",
        "print(\"\\n--- PROCESSING CADICA DATASET ---\")\n",
        "cadica_link = \"https://data.mendeley.com/public-api/zip/p9bpx9ctcv/download/5\"\n",
        "cadica_zip = \"cadica_outer.zip\"\n",
        "\n",
        "if not os.path.exists(\"selectedVideos\"):\n",
        "    print(\"Downloading CADICA...\")\n",
        "    run_cmd(f\"wget -q --show-progress -O {cadica_zip} {cadica_link}\")\n",
        "    run_cmd(f\"unzip -q {cadica_zip}\")\n",
        "    os.remove(cadica_zip)\n",
        "    inner_zips = glob.glob('**/*.zip', recursive=True)\n",
        "    if inner_zips:\n",
        "        run_cmd(f\"unzip -q '{inner_zips[0]}'\")\n",
        "        os.remove(inner_zips[0])\n",
        "\n",
        "cadica_roots = glob.glob('**/selectedVideos', recursive=True)\n",
        "if cadica_roots:\n",
        "    cadica_path = cadica_roots[0]\n",
        "    patients = [p for p in os.listdir(cadica_path) if os.path.isdir(os.path.join(cadica_path, p))]\n",
        "    cadica_tasks = []\n",
        "\n",
        "    for p_id in patients:\n",
        "        p_path = os.path.join(cadica_path, p_id)\n",
        "        videos = [v for v in os.listdir(p_path) if os.path.isdir(os.path.join(p_path, v))]\n",
        "\n",
        "        for v_id in videos:\n",
        "            input_path = os.path.join(p_path, v_id, 'input')\n",
        "            gt_path = os.path.join(p_path, v_id, 'groundtruth')\n",
        "            if not os.path.exists(input_path): continue\n",
        "\n",
        "            for frame_path in glob.glob(os.path.join(input_path, '*.*')):\n",
        "                frame_name = os.path.basename(frame_path)\n",
        "                gt_file = os.path.join(gt_path, f\"{os.path.splitext(frame_name)[0]}.txt\")\n",
        "                if not os.path.exists(gt_file):\n",
        "                    gt_file = None\n",
        "                cadica_tasks.append((frame_path, gt_file, p_id, v_id, frame_name))\n",
        "\n",
        "    for frame_path, gt_file, p_id, v_id, frame_name in tqdm(cadica_tasks, desc=\"CADICA All Images\"):\n",
        "        img = cv2.imread(frame_path)\n",
        "        if img is None: continue\n",
        "        h, w, _ = img.shape\n",
        "\n",
        "        new_img_name = f\"cadica_{p_id}_{v_id}_{frame_name}\"\n",
        "        new_lbl_name = new_img_name.rsplit('.', 1)[0] + '.txt'\n",
        "\n",
        "        # 1. Parse annotations first\n",
        "        annotations = []\n",
        "        if gt_file is not None and os.path.exists(gt_file):\n",
        "            with open(gt_file, 'r') as f_in:\n",
        "                for line in f_in.readlines():\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) >= 4:\n",
        "                        try:\n",
        "                            x, y, bw, bh = [float(val) for val in parts[:4]]\n",
        "                            xc, yc, nw, nh = to_yolo(w, h, x, y, bw, bh)\n",
        "                            annotations.append(f\"0 {xc:.6f} {yc:.6f} {nw:.6f} {nh:.6f}\\n\")\n",
        "                        except ValueError: continue\n",
        "\n",
        "        # 2. Check if the image contains stenosis\n",
        "        has_stenosis = len(annotations) > 0\n",
        "\n",
        "        # 3. Save to the ALL folder\n",
        "        save_image_and_labels(frame_path, IMG_ALL, LBL_ALL, new_img_name, new_lbl_name, annotations)\n",
        "\n",
        "        # 4. Save to the respective STENOSIS / NO_STENOSIS folders\n",
        "        if has_stenosis:\n",
        "            save_image_and_labels(frame_path, IMG_STENOSIS, LBL_STENOSIS, new_img_name, new_lbl_name, annotations)\n",
        "        else:\n",
        "            save_image_and_labels(frame_path, IMG_NO_STENOSIS, LBL_NO_STENOSIS, new_img_name, new_lbl_name, annotations)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. PROCESS ARCADE (ROBUST VERSION)\n",
        "# ==========================================\n",
        "print(\"\\n--- PROCESSING ARCADE DATASET ---\")\n",
        "\n",
        "import kagglehub\n",
        "arcade_base = kagglehub.dataset_download(\"nikitamanaenkov/annotated-x-ray-angiography-dataset\")\n",
        "print(f\"Path to dataset files: {arcade_base}\")\n",
        "\n",
        "# 1. Dynamically find all JSON files instead of hardcoding the paths\n",
        "json_paths = glob.glob(os.path.join(arcade_base, '**', '*.json'), recursive=True)\n",
        "\n",
        "if not json_paths:\n",
        "    print(\"-> [ERROR] Could not find ANY .json files in the ARCADE dataset! Check the download path.\")\n",
        "\n",
        "for json_path in json_paths:\n",
        "    # Deduce split name from the json filename (e.g., 'train.json' -> 'train')\n",
        "    split = os.path.basename(json_path).replace('.json', '')\n",
        "    print(f\"\\n-> Loading annotations from: {json_path}\")\n",
        "\n",
        "    # Deduce the image directory. Usually: parent_dir/annotations/train.json -> parent_dir/images\n",
        "    parent_dir = os.path.dirname(os.path.dirname(json_path))\n",
        "    img_dir = os.path.join(parent_dir, 'images')\n",
        "\n",
        "    if not os.path.exists(img_dir):\n",
        "        print(f\"-> [WARNING] Expected image folder not found at: {img_dir}. Skipping this split.\")\n",
        "        continue\n",
        "\n",
        "    with open(json_path, 'r') as f:\n",
        "        coco = json.load(f)\n",
        "\n",
        "    img_dict = {img['id']: {'name': img['file_name'], 'w': img['width'], 'h': img['height']} for img in coco['images']}\n",
        "\n",
        "    ann_dict = {}\n",
        "    for ann in coco.get('annotations', []):\n",
        "        ann_dict.setdefault(ann['image_id'], []).append(ann)\n",
        "\n",
        "    missing_images = 0\n",
        "    processed_count = 0\n",
        "\n",
        "    for img_id, img_info in tqdm(img_dict.items(), desc=f\"ARCADE {split.capitalize()}\"):\n",
        "        # os.path.basename fixes issues where COCO file_name is listed as \"images/img.png\"\n",
        "        clean_img_name = os.path.basename(img_info['name'])\n",
        "        orig_img_path = Path(img_dir) / clean_img_name\n",
        "\n",
        "        if not orig_img_path.exists():\n",
        "            missing_images += 1\n",
        "            continue\n",
        "\n",
        "        new_img_name = f\"arcade_{split}_{clean_img_name}\"\n",
        "        new_lbl_name = new_img_name.rsplit('.', 1)[0] + '.txt'\n",
        "\n",
        "        # Parse annotations\n",
        "        annotations = []\n",
        "        if img_id in ann_dict:\n",
        "            for ann in ann_dict[img_id]:\n",
        "                # COCO bbox is [x_min, y_min, width, height]\n",
        "                x_min, y_min, bw, bh = ann['bbox']\n",
        "                xc, yc, nw, nh = to_yolo(img_info['w'], img_info['h'], x_min, y_min, bw, bh)\n",
        "                annotations.append(f\"0 {xc:.6f} {yc:.6f} {nw:.6f} {nh:.6f}\\n\")\n",
        "\n",
        "        has_stenosis = len(annotations) > 0\n",
        "\n",
        "        # Save to ALL\n",
        "        save_image_and_labels(orig_img_path, IMG_ALL, LBL_ALL, new_img_name, new_lbl_name, annotations)\n",
        "\n",
        "        # Save to STENOSIS / NO_STENOSIS\n",
        "        if has_stenosis:\n",
        "            save_image_and_labels(orig_img_path, IMG_STENOSIS, LBL_STENOSIS, new_img_name, new_lbl_name, annotations)\n",
        "        else:\n",
        "            save_image_and_labels(orig_img_path, IMG_NO_STENOSIS, LBL_NO_STENOSIS, new_img_name, new_lbl_name, annotations)\n",
        "\n",
        "        processed_count += 1\n",
        "\n",
        "    if missing_images > 0:\n",
        "        print(f\"-> [WARNING] Skipped {missing_images} images because they could not be found on disk.\")\n",
        "    print(f\"-> Successfully processed {processed_count} images for {split}.\")"
      ],
      "metadata": {
        "id": "SWbO-iNJg-rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Define the dataset directories\n",
        "directories = {\n",
        "    \"All Data (Background + Stenosis)\": Path('fused_dataset_all'),\n",
        "    \"Stenosis Only\": Path('fused_dataset_stenosis_only'),\n",
        "    \"No Stenosis (Background Only)\": Path('fused_dataset_no_stenosis')\n",
        "}\n",
        "\n",
        "print(\"========================================\")\n",
        "print(\"          DATASET SUMMARY\")\n",
        "print(\"========================================\")\n",
        "\n",
        "for name, base_path in directories.items():\n",
        "    img_dir = base_path / 'images'\n",
        "    lbl_dir = base_path / 'labels'\n",
        "\n",
        "    # Check if directories exist to avoid errors\n",
        "    if not base_path.exists():\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"  -> [ERROR] Directory '{base_path}' does not exist.\")\n",
        "        continue\n",
        "\n",
        "    # Count files (images can be any extension, labels should be .txt)\n",
        "    num_images = len([f for f in img_dir.iterdir() if f.is_file()]) if img_dir.exists() else 0\n",
        "    num_labels = len(list(lbl_dir.glob('*.txt'))) if lbl_dir.exists() else 0\n",
        "\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  -> Path:   {base_path}\")\n",
        "    print(f\"  -> Images: {num_images}\")\n",
        "    print(f\"  -> Labels: {num_labels}\")\n",
        "\n",
        "    # Sanity check: Ensure every image has a corresponding label file\n",
        "    if num_images != num_labels:\n",
        "        print(f\"  -> [WARNING] Mismatch! {num_images} images vs {num_labels} labels.\")\n",
        "\n",
        "print(\"\\n========================================\")"
      ],
      "metadata": {
        "id": "ZYEThVSHdPo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "folders = [\n",
        "    'fused_dataset_all',\n",
        "    'fused_dataset_stenosis_only',\n",
        "]\n",
        "\n",
        "print(\"--- COMPRESSING DATASETS ---\")\n",
        "for folder in folders:\n",
        "    if os.path.exists(folder):\n",
        "        print(f\"Zipping {folder}...\")\n",
        "        # This creates a .zip file of the folder\n",
        "        shutil.make_archive(folder, 'zip', folder)\n",
        "        print(f\" -> Created {folder}.zip\")\n",
        "    else:\n",
        "        print(f\"[WARNING] {folder} does not exist.\")\n",
        "\n",
        "print(\"\\n--- DONE! ---\")\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "for folder in folders:\n",
        "    if os.path.exists(f\"{folder}.zip\"):\n",
        "        print(f\"Downloading {folder}.zip...\")\n",
        "        files.download(f\"{folder}.zip\")"
      ],
      "metadata": {
        "id": "MGAk7nqsh7YM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}