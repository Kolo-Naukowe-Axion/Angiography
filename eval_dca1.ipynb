{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SAM-VMNet Evaluation on DCA1 Test Set\n",
    "\n",
    "Quantitative evaluation of the VMUNet model trained on DCA1 dataset.\n",
    "Computes mIoU, Dice/F1, accuracy, sensitivity, specificity, and visualizes results.\n",
    "\n",
    "Also includes cross-dataset evaluation:\n",
    "- DCA1-trained model on ARCADE test set (generalization check)\n",
    "- ARCADE-trained model on DCA1 test set (baseline comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.insert(0, 'SAM_VMNet')\n",
    "\n",
    "from utils import set_seed, BceDiceLoss\n",
    "from models.vmunet.vmunet import VMUNet\n",
    "from dataset import Branch1_datasets\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Update these paths to point to your trained checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# DCA1-trained checkpoint\nDCA1_CKPT_PATH = './pre_trained_weights/dca1-best-epoch103-loss0.2863.pth'\n# ARCADE-trained checkpoint (baseline)\nARCADE_CKPT_PATH = './pre_trained_weights/best-epoch142-loss0.3230.pth'\n\nDCA1_DATA_PATH = './data/dca1/'\nARCADE_DATA_PATH = './data/vessel/'\n\nTHRESHOLD = 0.5"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "def load_model(ckpt_path, gpu_id='0'):\n    \"\"\"Load VMUNet with given checkpoint.\"\"\"\n    from configs.config_setting import setting_config\n    model_cfg = setting_config.model_config\n    model = VMUNet(\n        num_classes=model_cfg['num_classes'],\n        input_channels=model_cfg['input_channels'],\n        depths=model_cfg['depths'],\n        depths_decoder=model_cfg['depths_decoder'],\n        drop_path_rate=model_cfg['drop_path_rate'],\n        load_ckpt_path=model_cfg['load_ckpt_path'],\n        gpu_id=gpu_id,\n    )\n    model.load_from()\n\n    checkpoint = torch.load(ckpt_path, map_location='cpu')\n    if 'model_state_dict' in checkpoint:\n        state_dict = checkpoint['model_state_dict']\n    else:\n        state_dict = checkpoint\n    filtered = {k: v for k, v in state_dict.items()\n                if 'total_ops' not in k and 'total_params' not in k}\n    model.load_state_dict(filtered, strict=False)\n    model.eval()\n    print(f\"Loaded checkpoint: {ckpt_path}\")\n    return model\n\n\ndef evaluate_model(model, test_loader, criterion, threshold=0.5):\n    \"\"\"Run evaluation and return per-image predictions, GTs, and losses.\"\"\"\n    all_preds, all_gts, loss_list = [], [], []\n\n    model.eval()\n    with torch.no_grad():\n        for data in tqdm(test_loader, desc=\"Evaluating\"):\n            img, msk = data\n            img, msk = img.float(), msk.float()\n            out = model(img)\n            loss = criterion(out.cpu(), msk)\n            loss_list.append(loss.item())\n\n            msk_np = msk.squeeze(1).cpu().detach().numpy()\n            if type(out) is tuple:\n                out = out[0]\n            out_np = out.squeeze(1).cpu().detach().numpy()\n            all_preds.append(out_np)\n            all_gts.append(msk_np)\n\n    return all_preds, all_gts, loss_list\n\n\ndef compute_metrics(all_preds, all_gts, loss_list, threshold=0.5):\n    \"\"\"Compute aggregate and per-image metrics.\"\"\"\n    preds_flat = np.array(all_preds).reshape(-1)\n    gts_flat = np.array(all_gts).reshape(-1)\n\n    y_pre = np.where(preds_flat >= threshold, 1, 0)\n    y_true = np.where(gts_flat >= 0.5, 1, 0)\n\n    confusion = confusion_matrix(y_true, y_pre)\n    TN, FP, FN, TP = confusion[0, 0], confusion[0, 1], confusion[1, 0], confusion[1, 1]\n\n    accuracy = float(TN + TP) / float(np.sum(confusion)) if float(np.sum(confusion)) != 0 else 0\n    sensitivity = float(TP) / float(TP + FN) if float(TP + FN) != 0 else 0\n    specificity = float(TN) / float(TN + FP) if float(TN + FP) != 0 else 0\n    f1_or_dsc = float(2 * TP) / float(2 * TP + FP + FN) if float(2 * TP + FP + FN) != 0 else 0\n    miou = float(TP) / float(TP + FP + FN) if float(TP + FP + FN) != 0 else 0\n\n    # Per-image metrics\n    per_image_dice, per_image_miou = [], []\n    for i in range(len(all_preds)):\n        pred_flat = all_preds[i].reshape(-1)\n        gt_flat = all_gts[i].reshape(-1)\n        y_p = np.where(pred_flat >= threshold, 1, 0)\n        y_t = np.where(gt_flat >= 0.5, 1, 0)\n        cm = confusion_matrix(y_t, y_p, labels=[0, 1])\n        tn, fp, fn, tp = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n        dice = float(2 * tp) / float(2 * tp + fp + fn) if float(2 * tp + fp + fn) != 0 else 1.0\n        iou = float(tp) / float(tp + fp + fn) if float(tp + fp + fn) != 0 else 1.0\n        per_image_dice.append(dice)\n        per_image_miou.append(iou)\n\n    return {\n        'loss': np.mean(loss_list),\n        'miou': miou, 'dice': f1_or_dsc,\n        'accuracy': accuracy, 'sensitivity': sensitivity, 'specificity': specificity,\n        'TN': TN, 'FP': FP, 'FN': FN, 'TP': TP,\n        'per_image_dice': np.array(per_image_dice),\n        'per_image_miou': np.array(per_image_miou),\n    }\n\n\ndef print_metrics(metrics, title):\n    \"\"\"Print formatted metrics summary.\"\"\"\n    print(\"=\" * 60)\n    print(f\"  {title}\")\n    print(\"=\" * 60)\n    print(f\"  Loss:         {metrics['loss']:.4f}\")\n    print(f\"  mIoU:         {metrics['miou']:.4f}\")\n    print(f\"  Dice/F1:      {metrics['dice']:.4f}\")\n    print(f\"  Accuracy:     {metrics['accuracy']:.4f}\")\n    print(f\"  Sensitivity:  {metrics['sensitivity']:.4f}\")\n    print(f\"  Specificity:  {metrics['specificity']:.4f}\")\n    print(f\"  Per-Image Dice: {np.mean(metrics['per_image_dice']):.4f} +/- {np.std(metrics['per_image_dice']):.4f}\")\n    print(f\"  Per-Image mIoU: {np.mean(metrics['per_image_miou']):.4f} +/- {np.std(metrics['per_image_miou']):.4f}\")\n    print(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 1. Evaluate DCA1-Trained Model on DCA1 Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config_setting_dca1 import setting_config as dca1_config\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dca1_config.gpu_id = '0'\n",
    "\n",
    "# Load DCA1 test set\n",
    "dca1_test_dataset = Branch1_datasets(DCA1_DATA_PATH, dca1_config, train=False, test=True)\n",
    "dca1_test_loader = DataLoader(dca1_test_dataset, batch_size=1, shuffle=False)\n",
    "print(f\"DCA1 test set: {len(dca1_test_dataset)} images\")\n",
    "\n",
    "criterion = BceDiceLoss(wb=1, wd=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DCA1-trained model\n",
    "dca1_model = load_model(DCA1_CKPT_PATH)\n",
    "\n",
    "# Evaluate\n",
    "dca1_preds, dca1_gts, dca1_losses = evaluate_model(dca1_model, dca1_test_loader, criterion)\n",
    "dca1_metrics = compute_metrics(dca1_preds, dca1_gts, dca1_losses)\n",
    "print_metrics(dca1_metrics, 'DCA1-Trained Model on DCA1 Test Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 2. Per-Image Metrics Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Dice histogram\n",
    "axes[0].hist(dca1_metrics['per_image_dice'], bins=15, color='steelblue', edgecolor='white', alpha=0.85)\n",
    "axes[0].axvline(np.mean(dca1_metrics['per_image_dice']), color='red', linestyle='--', linewidth=2,\n",
    "                label=f\"Mean={np.mean(dca1_metrics['per_image_dice']):.4f}\")\n",
    "axes[0].axvline(np.median(dca1_metrics['per_image_dice']), color='orange', linestyle='--', linewidth=2,\n",
    "                label=f\"Median={np.median(dca1_metrics['per_image_dice']):.4f}\")\n",
    "axes[0].set_xlabel('Dice Score', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Per-Image Dice Score Distribution (DCA1)', fontsize=13)\n",
    "axes[0].legend(fontsize=10)\n",
    "\n",
    "# mIoU histogram\n",
    "axes[1].hist(dca1_metrics['per_image_miou'], bins=15, color='darkorange', edgecolor='white', alpha=0.85)\n",
    "axes[1].axvline(np.mean(dca1_metrics['per_image_miou']), color='red', linestyle='--', linewidth=2,\n",
    "                label=f\"Mean={np.mean(dca1_metrics['per_image_miou']):.4f}\")\n",
    "axes[1].axvline(np.median(dca1_metrics['per_image_miou']), color='blue', linestyle='--', linewidth=2,\n",
    "                label=f\"Median={np.median(dca1_metrics['per_image_miou']):.4f}\")\n",
    "axes[1].set_xlabel('mIoU', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Per-Image mIoU Distribution (DCA1)', fontsize=13)\n",
    "axes[1].legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "cm_display = np.array([[dca1_metrics['TN'], dca1_metrics['FP']],\n",
    "                       [dca1_metrics['FN'], dca1_metrics['TP']]])\n",
    "cm_normalized = cm_display.astype(float) / cm_display.sum()\n",
    "\n",
    "im = ax.imshow(cm_normalized, cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar(im, ax=ax, label='Proportion')\n",
    "\n",
    "labels = ['Background (0)', 'Vessel (1)']\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(labels, fontsize=11)\n",
    "ax.set_yticklabels(labels, fontsize=11)\n",
    "ax.set_xlabel('Predicted', fontsize=13)\n",
    "ax.set_ylabel('Actual', fontsize=13)\n",
    "ax.set_title('Confusion Matrix â€” DCA1-Trained on DCA1 Test', fontsize=14)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        count = cm_display[i, j]\n",
    "        pct = cm_normalized[i, j] * 100\n",
    "        color = 'white' if cm_normalized[i, j] > 0.5 else 'black'\n",
    "        ax.text(j, i, f'{count:,}\\n({pct:.2f}%)',\n",
    "                ha='center', va='center', fontsize=12, color=color, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 4. Best and Worst Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(dca1_metrics['per_image_dice'])\n",
    "worst_3 = sorted_indices[:3]\n",
    "best_3 = sorted_indices[-3:][::-1]\n",
    "\n",
    "def show_cases(indices, title, test_dataset, all_preds, per_image_dice, threshold=0.5):\n",
    "    fig, axes = plt.subplots(len(indices), 3, figsize=(14, 4.5 * len(indices)))\n",
    "    if len(indices) == 1:\n",
    "        axes = axes[np.newaxis, :]\n",
    "\n",
    "    for row, idx in enumerate(indices):\n",
    "        img_path, msk_path = test_dataset.data[idx]\n",
    "        raw_img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        raw_msk = np.array(Image.open(msk_path).convert('L'))\n",
    "\n",
    "        gt_resized = np.array(Image.fromarray(raw_msk).resize((256, 256), Image.NEAREST))\n",
    "        gt_binary = (gt_resized / 255.0 >= 0.5).astype(np.float32)\n",
    "        pred_binary = np.where(all_preds[idx].squeeze() >= threshold, 1, 0).astype(np.float32)\n",
    "\n",
    "        axes[row, 0].imshow(raw_img)\n",
    "        axes[row, 0].set_title(f'{os.path.basename(img_path)}', fontsize=10)\n",
    "        axes[row, 0].axis('off')\n",
    "\n",
    "        axes[row, 1].imshow(gt_binary, cmap='gray')\n",
    "        axes[row, 1].set_title('Ground Truth', fontsize=10)\n",
    "        axes[row, 1].axis('off')\n",
    "\n",
    "        axes[row, 2].imshow(pred_binary, cmap='gray')\n",
    "        axes[row, 2].set_title(f'Prediction (Dice={per_image_dice[idx]:.4f})', fontsize=10)\n",
    "        axes[row, 2].axis('off')\n",
    "\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold', y=1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_cases(best_3, 'Top 3 Best Predictions (DCA1)', dca1_test_dataset, dca1_preds, dca1_metrics['per_image_dice'])\n",
    "show_cases(worst_3, 'Top 3 Worst Predictions (DCA1)', dca1_test_dataset, dca1_preds, dca1_metrics['per_image_dice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Cross-Dataset Evaluation\n",
    "\n",
    "### 5a. ARCADE-Trained Model on DCA1 Test Set (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "arcade_model = load_model(ARCADE_CKPT_PATH)\n",
    "\n",
    "arcade_on_dca1_preds, arcade_on_dca1_gts, arcade_on_dca1_losses = evaluate_model(\n",
    "    arcade_model, dca1_test_loader, criterion)\n",
    "arcade_on_dca1_metrics = compute_metrics(arcade_on_dca1_preds, arcade_on_dca1_gts, arcade_on_dca1_losses)\n",
    "print_metrics(arcade_on_dca1_metrics, 'ARCADE-Trained Model on DCA1 Test Set (Baseline)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### 5b. DCA1-Trained Model on ARCADE Test Set (Generalization Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config_setting import setting_config as arcade_config\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    arcade_config.gpu_id = '0'\n",
    "\n",
    "# Load ARCADE test set\n",
    "if os.path.exists(ARCADE_DATA_PATH):\n",
    "    arcade_test_dataset = Branch1_datasets(ARCADE_DATA_PATH, arcade_config, train=False, test=True)\n",
    "    arcade_test_loader = DataLoader(arcade_test_dataset, batch_size=1, shuffle=False)\n",
    "    print(f\"ARCADE test set: {len(arcade_test_dataset)} images\")\n",
    "\n",
    "    dca1_on_arcade_preds, dca1_on_arcade_gts, dca1_on_arcade_losses = evaluate_model(\n",
    "        dca1_model, arcade_test_loader, criterion)\n",
    "    dca1_on_arcade_metrics = compute_metrics(dca1_on_arcade_preds, dca1_on_arcade_gts, dca1_on_arcade_losses)\n",
    "    print_metrics(dca1_on_arcade_metrics, 'DCA1-Trained Model on ARCADE Test Set')\n",
    "else:\n",
    "    print(f\"ARCADE data not found at {ARCADE_DATA_PATH}, skipping cross-dataset eval.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### 5c. Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"  CROSS-DATASET COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  {'Experiment':<45} {'Dice':>8} {'mIoU':>8}\")\n",
    "print(f\"  {'-'*45} {'-'*8} {'-'*8}\")\n",
    "print(f\"  {'DCA1-trained on DCA1 test':<45} {dca1_metrics['dice']:>8.4f} {dca1_metrics['miou']:>8.4f}\")\n",
    "print(f\"  {'ARCADE-trained on DCA1 test (baseline)':<45} {arcade_on_dca1_metrics['dice']:>8.4f} {arcade_on_dca1_metrics['miou']:>8.4f}\")\n",
    "if os.path.exists(ARCADE_DATA_PATH):\n",
    "    print(f\"  {'DCA1-trained on ARCADE test (generalization)':<45} {dca1_on_arcade_metrics['dice']:>8.4f} {dca1_on_arcade_metrics['miou']:>8.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}